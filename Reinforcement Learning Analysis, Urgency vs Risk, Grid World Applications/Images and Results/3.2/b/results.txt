//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,167652,27789,2398483,452487,1255,1947,2279,5674,2618,2284,904,1323,48,48,44,35,40,45,33,35,35,38,48,41,40,49,39,44,50,38,33,36,42,42,55,49,58,51,56,58,48,47,44,39,51,46,53,44,45,47,49,59,46,40,58,41,47,57,45,54,46,47,40,53,46,47,51,42,48,60,50,41,40,45,48,43,44,51,47,53,45,58,56,41,54,41,45,54,42,56,58,37,51,51,44,53,47,62,47,50
Policy Iteration,10978,58447,445997,326418,26351,495092,31594,68894,12782,1418,3130,821,2150,377,2361,978,69,34,35,48,39,38,39,41,47,42,41,42,33,32,35,33,30,54,35,53,47,55,58,40,60,44,46,54,53,51,54,54,51,48,38,51,42,47,49,42,47,48,46,43,50,53,47,50,43,49,46,47,55,50,42,41,47,46,38,50,54,52,42,40,44,45,50,54,47,47,36,40,49,38,57,44,49,40,49,57,46,51,40,45
Q Learning,621,703,308,311,583,981,640,922,274,442,852,323,615,594,324,421,795,257,267,219,144,286,302,105,112,263,182,200,506,128,618,134,69,553,131,215,287,573,138,41,115,124,45,106,106,114,216,67,51,97,174,162,102,113,226,77,141,240,77,82,73,215,665,207,224,140,117,505,120,156,327,54,178,87,363,350,80,64,177,92,414,254,96,47,336,43,283,99,295,77,120,70,53,112,530,156,80,155,92,74

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,68,13,8,11,15,17,19,21,25,37,243,30,32,35,38,43,51,46,42,48,54,54,229,54,59,61,61,66,68,72,241,69,72,82,88,80,120,261,84,91,89,102,283,91,100,91,97,244,94,102,102,243,102,104,113,232,111,113,117,225,116,128,128,224,124,131,218,130,137,144,219,144,142,213,152,160,147,215,161,146,198,168,157,172,172,182,169,178,186,180,188,182,200,227,198,215,225,215,213,195
Policy Iteration,19,12,12,16,20,22,26,29,34,38,152,43,48,50,52,57,62,85,69,229,74,75,79,84,89,92,97,98,100,111,123,111,130,117,142,121,129,144,149,175,143,165,161,155,168,156,160,163,173,184,185,188,223,206,235,237,197,227,232,203,201,227,256,266,222,225,264,251,279,278,254,283,249,252,253,259,259,264,292,286,270,279,290,324,316,293,320,334,304,330,372,339,329,315,346,361,343,360,329,334
Q Learning,24,36,32,23,34,36,27,40,31,41,27,34,28,32,34,37,39,30,34,28,39,32,39,38,36,43,49,44,45,38,47,38,41,46,52,45,47,60,50,51,41,44,41,41,48,51,55,52,51,47,44,52,57,51,56,48,52,53,58,58,65,53,52,60,63,83,67,68,58,57,68,64,72,54,57,68,58,63,66,70,66,75,76,69,63,57,91,71,61,65,73,73,88,68,80,85,65,80,97,63

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-167848.0,-27687.0,-2398381.0,-452385.0,-1153.0,-1845.0,-2177.0,-5572.0,-2516.0,-2182.0,-802.0,-1221.0,-95.0,54.0,-91.0,67.0,62.0,57.0,69.0,67.0,67.0,64.0,54.0,61.0,62.0,-96.0,-86.0,58.0,52.0,64.0,69.0,66.0,60.0,60.0,47.0,53.0,44.0,51.0,46.0,44.0,54.0,55.0,58.0,63.0,51.0,56.0,49.0,58.0,57.0,55.0,53.0,43.0,56.0,62.0,44.0,61.0,55.0,45.0,57.0,48.0,56.0,55.0,62.0,49.0,56.0,55.0,51.0,60.0,54.0,42.0,52.0,61.0,62.0,57.0,54.0,59.0,58.0,51.0,55.0,49.0,57.0,44.0,46.0,61.0,48.0,61.0,57.0,48.0,60.0,46.0,44.0,65.0,51.0,51.0,58.0,49.0,55.0,40.0,55.0,52.0
Policy Iteration Rewards,-11025.0,-58494.0,-445895.0,-326316.0,-26249.0,-494990.0,-31492.0,-68792.0,-12829.0,-1316.0,-3028.0,-719.0,-2048.0,-275.0,-2259.0,-876.0,33.0,68.0,67.0,54.0,63.0,64.0,63.0,61.0,55.0,60.0,-88.0,-89.0,69.0,70.0,67.0,69.0,72.0,48.0,67.0,49.0,55.0,47.0,44.0,62.0,42.0,58.0,56.0,48.0,49.0,51.0,48.0,48.0,51.0,54.0,64.0,51.0,60.0,55.0,53.0,60.0,55.0,54.0,56.0,59.0,52.0,49.0,55.0,52.0,59.0,53.0,56.0,55.0,47.0,52.0,60.0,61.0,55.0,56.0,64.0,52.0,48.0,50.0,60.0,62.0,58.0,57.0,52.0,48.0,55.0,55.0,66.0,62.0,53.0,64.0,45.0,58.0,53.0,62.0,53.0,45.0,56.0,51.0,62.0,57.0
Q Learning Rewards,-966.0,-601.0,-206.0,-209.0,-779.0,-1475.0,-538.0,-1267.0,-172.0,-340.0,-750.0,-221.0,-513.0,-641.0,-222.0,-319.0,-842.0,-453.0,-314.0,-564.0,-42.0,-184.0,-498.0,-3.0,-10.0,-161.0,-80.0,-98.0,-404.0,-324.0,-814.0,-32.0,33.0,-1196.0,-29.0,-113.0,-185.0,-769.0,-185.0,61.0,-13.0,-22.0,57.0,-4.0,-4.0,-12.0,-114.0,35.0,-98.0,5.0,-72.0,-60.0,0.0,-11.0,-124.0,25.0,-188.0,-138.0,25.0,20.0,29.0,-858.0,-563.0,-105.0,-122.0,-38.0,-15.0,-403.0,-18.0,-54.0,-225.0,48.0,-672.0,15.0,-857.0,-248.0,22.0,38.0,-75.0,10.0,-610.0,-152.0,6.0,55.0,-234.0,-90.0,-181.0,3.0,-342.0,25.0,-18.0,32.0,49.0,-10.0,-1173.0,-203.0,22.0,-53.0,10.0,28.0
