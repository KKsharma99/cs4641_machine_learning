//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,16914,5201,16276,4862,9722,2555,3141,2058,1727,352,5503,5035,1597,2676,1095,51,44,35,35,37,36,37,43,41,46,31,38,44,34,35,34,40,35,33,41,55,34,40,42,39,42,43,38,38,40,45,33,35,46,40,39,36,41,41,44,39,49,41,40,39,44,34,41,34,47,37,35,41,36,33,33,45,43,33,44,40,41,43,47,44,37,33,33,39,39,30,43,44,36,46,38,37,46,34,34,41,31,36,32,41
Policy Iteration,627453,54000,41879,15474,3202,4564,2215,1056,1794,2020,2282,8919,669,3335,2593,481,716,44,37,40,41,45,32,39,43,44,37,38,40,34,37,45,40,39,51,45,41,32,43,40,37,32,41,33,36,46,34,35,40,35,36,40,38,36,37,37,45,33,40,40,47,35,35,49,39,38,36,37,38,44,46,36,35,41,37,48,47,39,33,46,38,46,31,39,34,34,34,31,41,41,41,29,34,43,37,48,33,36,33,39
Q Learning,

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,55,8,8,9,11,14,16,28,35,37,41,25,27,31,31,32,35,35,39,39,41,46,45,48,50,52,52,52,75,94,99,104,110,110,118,87,77,107,90,85,81,81,79,84,81,91,95,88,93,94,95,98,98,98,102,124,175,170,108,109,108,112,113,117,117,131,131,132,131,126,136,133,133,133,134,141,141,146,142,149,145,138,143,155,151,153,155,158,161,160,160,160,168,170,170,186,172,177,181,179
Policy Iteration,13,9,11,15,18,27,25,28,31,36,40,42,51,48,56,56,60,61,64,67,70,73,82,84,84,85,90,94,97,100,104,106,110,115,117,119,124,132,130,139,141,144,143,149,156,156,159,160,163,179,189,169,184,188,186,194,190,186,205,196,211,225,214,214,222,216,224,225,227,229,241,316,284,255,244,244,250,253,250,278,288,358,333,269,292,287,286,286,298,293,296,308,306,312,316,309,317,322,326,323
Q Learning,

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-84475.0,-25895.0,-81270.0,-24200.0,-48500.0,-12665.0,-15595.0,-10180.0,-8525.0,-1650.0,-27405.0,-25065.0,-7875.0,-13270.0,-5365.0,-145.0,-110.0,-65.0,-65.0,-75.0,-70.0,-75.0,-105.0,-95.0,-120.0,-45.0,-80.0,-110.0,-60.0,-65.0,-65.0,-95.0,-65.0,-55.0,-95.0,-165.0,-60.0,-90.0,-100.0,-85.0,-100.0,-105.0,-80.0,-80.0,-90.0,-115.0,-55.0,-65.0,-125.0,-95.0,-85.0,-70.0,-95.0,-95.0,-110.0,-90.0,-140.0,-95.0,-90.0,-85.0,-110.0,-60.0,-95.0,-60.0,-125.0,-75.0,-65.0,-95.0,-70.0,-55.0,-60.0,-115.0,-105.0,-55.0,-110.0,-90.0,-95.0,-105.0,-125.0,-110.0,-75.0,-55.0,-55.0,-85.0,-85.0,-40.0,-105.0,-110.0,-70.0,-120.0,-80.0,-75.0,-120.0,-60.0,-60.0,-95.0,-50.0,-75.0,-50.0,-95.0
Policy Iteration Rewards,-3137175.0,-269890.0,-209285.0,-77260.0,-15900.0,-22710.0,-10965.0,-5170.0,-8865.0,-9990.0,-11305.0,-44485.0,-3235.0,-16565.0,-12855.0,-2295.0,-3470.0,-110.0,-75.0,-90.0,-100.0,-120.0,-50.0,-90.0,-105.0,-110.0,-75.0,-80.0,-90.0,-60.0,-75.0,-125.0,-90.0,-85.0,-145.0,-120.0,-95.0,-50.0,-105.0,-90.0,-75.0,-50.0,-95.0,-55.0,-70.0,-120.0,-60.0,-65.0,-90.0,-65.0,-70.0,-90.0,-80.0,-70.0,-80.0,-75.0,-115.0,-55.0,-95.0,-90.0,-125.0,-65.0,-65.0,-140.0,-95.0,-80.0,-70.0,-75.0,-80.0,-110.0,-125.0,-70.0,-65.0,-95.0,-75.0,-130.0,-125.0,-90.0,-55.0,-120.0,-80.0,-120.0,-45.0,-85.0,-60.0,-60.0,-60.0,-50.0,-95.0,-95.0,-95.0,-35.0,-65.0,-105.0,-75.0,-130.0,-55.0,-70.0,-55.0,-85.0
Q Learning Rewards,
//Q Learning Analysis//
Q Learning,1762,431,98,284,213,239,1056,816,248,304,128,395,89,396,197,174,99,113,268,75,240,645,470,216,134,145,116,79,398,359,234,757,69,88,110,201,123,145,42,59,157,94,304,147,81,55,98,158,138,51,123,144,145,70,63,119,104,116,167,65,154,139,59,102,47,64,37,64,65,65,87,94,72,41,111,85,145,74,78,118,140,140,120,153,104,175,262,144,53,77,86,113,75,61,49,47,59,137,52,196
Starting reachability analysis
Finished reachability analysis; # states: 151
Passes: 31

This is your optimal policy:
num of rows in policy is 15
[v,*,^,<,*,*,*,*,>,*,v,>,>,>,^]
[<,>,<,^,*,*,*,*,v,*,>,<,^,^,<]
[^,^,*,v,v,v,*,*,v,*,*,>,*,^,<]
[<,>,*,<,*,*,>,v,v,*,>,>,>,^,^]
[^,^,*,>,>,>,^,*,>,^,^,*,^,*,*]
[v,v,*,*,*,*,*,>,>,>,^,*,^,v,v]
[v,v,*,*,*,*,>,v,v,*,*,*,^,<,v]
[<,v,*,*,>,>,>,>,<,*,*,*,*,^,<]
[^,v,*,>,<,>,^,>,v,*,*,*,>,^,<]
[>,>,>,^,v,^,*,*,*,*,*,*,^,^,^]
[<,>,>,*,*,*,>,>,v,*,*,*,v,^,^]
[^,^,*,*,^,>,^,*,v,^,*,*,^,*,*]
[^,^,<,>,>,^,*,*,>,v,v,^,^,<,<]
[^,<,^,^,<,^,*,*,>,>,^,>,^,v,^]
[^,^,<,^,>,^,*,*,<,v,^,v,<,v,^]



Num generated: 2220; num unique: 151
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,16914,5201,16276,4862,9722,2555,3141,2058,1727,352,5503,5035,1597,2676,1095,51,44,35,35,37,36,37,43,41,46,31,38,44,34,35,34,40,35,33,41,55,34,40,42,39,42,43,38,38,40,45,33,35,46,40,39,36,41,41,44,39,49,41,40,39,44,34,41,34,47,37,35,41,36,33,33,45,43,33,44,40,41,43,47,44,37,33,33,39,39,30,43,44,36,46,38,37,46,34,34,41,31,36,32,41
Policy Iteration,627453,54000,41879,15474,3202,4564,2215,1056,1794,2020,2282,8919,669,3335,2593,481,716,44,37,40,41,45,32,39,43,44,37,38,40,34,37,45,40,39,51,45,41,32,43,40,37,32,41,33,36,46,34,35,40,35,36,40,38,36,37,37,45,33,40,40,47,35,35,49,39,38,36,37,38,44,46,36,35,41,37,48,47,39,33,46,38,46,31,39,34,34,34,31,41,41,41,29,34,43,37,48,33,36,33,39
Q Learning,1762,431,98,284,213,239,1056,816,248,304,128,395,89,396,197,174,99,113,268,75,240,645,470,216,134,145,116,79,398,359,234,757,69,88,110,201,123,145,42,59,157,94,304,147,81,55,98,158,138,51,123,144,145,70,63,119,104,116,167,65,154,139,59,102,47,64,37,64,65,65,87,94,72,41,111,85,145,74,78,118,140,140,120,153,104,175,262,144,53,77,86,113,75,61,49,47,59,137,52,196

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,55,8,8,9,11,14,16,28,35,37,41,25,27,31,31,32,35,35,39,39,41,46,45,48,50,52,52,52,75,94,99,104,110,110,118,87,77,107,90,85,81,81,79,84,81,91,95,88,93,94,95,98,98,98,102,124,175,170,108,109,108,112,113,117,117,131,131,132,131,126,136,133,133,133,134,141,141,146,142,149,145,138,143,155,151,153,155,158,161,160,160,160,168,170,170,186,172,177,181,179
Policy Iteration,13,9,11,15,18,27,25,28,31,36,40,42,51,48,56,56,60,61,64,67,70,73,82,84,84,85,90,94,97,100,104,106,110,115,117,119,124,132,130,139,141,144,143,149,156,156,159,160,163,179,189,169,184,188,186,194,190,186,205,196,211,225,214,214,222,216,224,225,227,229,241,316,284,255,244,244,250,253,250,278,288,358,333,269,292,287,286,286,298,293,296,308,306,312,316,309,317,322,326,323
Q Learning,41,14,15,29,23,25,19,23,21,29,31,29,25,25,22,27,24,26,30,30,30,38,33,34,36,31,32,33,30,41,40,36,38,38,38,39,38,38,40,39,37,38,42,46,38,52,47,41,46,37,42,45,45,47,42,45,52,43,50,47,53,46,54,42,44,48,52,51,47,44,48,50,53,93,53,49,52,48,60,53,54,50,57,51,48,49,59,55,52,77,53,53,58,49,62,55,52,55,59,67

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-84475.0,-25895.0,-81270.0,-24200.0,-48500.0,-12665.0,-15595.0,-10180.0,-8525.0,-1650.0,-27405.0,-25065.0,-7875.0,-13270.0,-5365.0,-145.0,-110.0,-65.0,-65.0,-75.0,-70.0,-75.0,-105.0,-95.0,-120.0,-45.0,-80.0,-110.0,-60.0,-65.0,-65.0,-95.0,-65.0,-55.0,-95.0,-165.0,-60.0,-90.0,-100.0,-85.0,-100.0,-105.0,-80.0,-80.0,-90.0,-115.0,-55.0,-65.0,-125.0,-95.0,-85.0,-70.0,-95.0,-95.0,-110.0,-90.0,-140.0,-95.0,-90.0,-85.0,-110.0,-60.0,-95.0,-60.0,-125.0,-75.0,-65.0,-95.0,-70.0,-55.0,-60.0,-115.0,-105.0,-55.0,-110.0,-90.0,-95.0,-105.0,-125.0,-110.0,-75.0,-55.0,-55.0,-85.0,-85.0,-40.0,-105.0,-110.0,-70.0,-120.0,-80.0,-75.0,-120.0,-60.0,-60.0,-95.0,-50.0,-75.0,-50.0,-95.0
Policy Iteration Rewards,-3137175.0,-269890.0,-209285.0,-77260.0,-15900.0,-22710.0,-10965.0,-5170.0,-8865.0,-9990.0,-11305.0,-44485.0,-3235.0,-16565.0,-12855.0,-2295.0,-3470.0,-110.0,-75.0,-90.0,-100.0,-120.0,-50.0,-90.0,-105.0,-110.0,-75.0,-80.0,-90.0,-60.0,-75.0,-125.0,-90.0,-85.0,-145.0,-120.0,-95.0,-50.0,-105.0,-90.0,-75.0,-50.0,-95.0,-55.0,-70.0,-120.0,-60.0,-65.0,-90.0,-65.0,-70.0,-90.0,-80.0,-70.0,-80.0,-75.0,-115.0,-55.0,-95.0,-90.0,-125.0,-65.0,-65.0,-140.0,-95.0,-80.0,-70.0,-75.0,-80.0,-110.0,-125.0,-70.0,-65.0,-95.0,-75.0,-130.0,-125.0,-90.0,-55.0,-120.0,-80.0,-120.0,-45.0,-85.0,-60.0,-60.0,-60.0,-50.0,-95.0,-95.0,-95.0,-35.0,-65.0,-105.0,-75.0,-130.0,-55.0,-70.0,-55.0,-85.0
Q Learning Rewards,-8760.0,-2045.0,-385.0,-1325.0,-955.0,-1085.0,-5170.0,-3970.0,-1130.0,-1435.0,-535.0,-1865.0,-340.0,-1870.0,-895.0,-770.0,-385.0,-455.0,-1230.0,-280.0,-1090.0,-3115.0,-2240.0,-970.0,-565.0,-615.0,-470.0,-290.0,-1925.0,-1685.0,-1060.0,-3675.0,-240.0,-335.0,-440.0,-900.0,-515.0,-630.0,-105.0,-195.0,-675.0,-360.0,-1410.0,-625.0,-300.0,-180.0,-410.0,-680.0,-585.0,-155.0,-505.0,-610.0,-615.0,-240.0,-210.0,-485.0,-425.0,-470.0,-725.0,-215.0,-660.0,-585.0,-185.0,-400.0,-125.0,-210.0,-75.0,-210.0,-215.0,-215.0,-330.0,-360.0,-250.0,-95.0,-460.0,-355.0,-625.0,-265.0,-285.0,-480.0,-595.0,-600.0,-495.0,-670.0,-410.0,-765.0,-1200.0,-610.0,-160.0,-275.0,-320.0,-455.0,-265.0,-200.0,-135.0,-125.0,-185.0,-580.0,-155.0,-895.0
